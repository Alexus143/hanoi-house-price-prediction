# src/preprocessing/cleaner.py
import pandas as pd
import sqlite3
import os
import numpy as np
import sys
import re
from datetime import datetime
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from src import config

# --- HÃ€M Láº¤Y PHÆ¯á»œNG Tá»ª Äá»ŠA CHá»ˆ ---
def extract_ward(location_str):
    """Input: "YÃªn NghÄ©a, HÃ  ÄÃ´ng" -> Output: "YÃªn NghÄ©a" """
    if pd.isna(location_str): return "KhÃ¡c"
    parts = location_str.split(',') 
    if len(parts) > 1:
        return parts[0].strip()
    return location_str.strip()

def clean_price(price_str):
    """Chuyá»ƒn Ä‘á»•i giÃ¡ sang sá»‘ thá»±c (Tá»· VNÄ)"""
    if pd.isna(price_str): return None
    price_str = str(price_str).lower().replace(',', '.') 
    if "tá»·" in price_str:
        return float(price_str.replace("tá»·", "").strip())
    elif "triá»‡u" in price_str:
        return float(price_str.replace("triá»‡u", "").strip()) / 1000
    return None

def is_misplaced_text(val):
    """Nháº­n diá»‡n description Ä‘i láº¡c vÃ o published_date"""
    if pd.isna(val): return False
    val = str(val).strip()
    # Náº¿u khÃ´ng pháº£i dd/mm/yyyy vÃ  dÃ i hÆ¡n 10 kÃ½ tá»± -> Text Ä‘i láº¡c
    if not re.match(r'^\d{2}/\d{2}/\d{4}$', val) and len(val) > 10:
        return True
    return False

def clean_description(text):
    """XÃ³a cÃ¡c ngÃ y thÃ¡ng rÃ¡c bá»‹ rÆ¡i vÃ o cá»™t description"""
    if pd.isna(text): return ""
    text = str(text).strip()
    if re.match(r'^\d{2}/\d{2}/\d{4}$', text):
        return ""
    return text

def process_and_save():
    if not os.path.exists(config.RAW_CSV_PATH):
        print(f"KhÃ´ng tÃ¬m tháº¥y file táº¡i: {config.RAW_CSV_PATH}")
        return

    column_names = ['title', 'price', 'area', 'location', 'scraped_date', 'published_date', 'description']
    print(f"Äang Ä‘á»c dá»¯ liá»‡u thÃ´...")

    try:
        df = pd.read_csv(
            config.RAW_CSV_PATH, header=None, names=column_names, 
            skiprows=1, on_bad_lines='skip', engine='python'
        )
        # XÃ³a dÃ²ng header rÃ¡c bá»‹ láº·p
        df = df[df['title'] != 'title']
        print(f"Tá»•ng sá»‘ dÃ²ng thÃ´: {len(df)}")
    except Exception as e:
        print(f"Lá»—i Ä‘á»c CSV: {e}")
        return

    # --- 1. LÃ€M Sáº CH CÆ  Báº¢N ---
    df['price_billion'] = df['price'].apply(clean_price)
    df['area'] = df['area'].astype(str).str.extract(r'(\d+\.?\d*)').astype(float)
    df['ward'] = df['location'].apply(extract_ward)
    df.replace(r'^\s*$', np.nan, regex=True, inplace=True)

    # --- 2. SELF-HEALING: Sá»¬A Lá»–I Lá»˜N Xá»˜N Cá»˜T ---
    if 'published_date' in df.columns:
        mask_swapped = df['published_date'].apply(is_misplaced_text)
        if mask_swapped.sum() > 0:
            print(f"ğŸ”„ Äang phá»¥c há»“i {mask_swapped.sum()} dÃ²ng bá»‹ láº«n text vÃ o published_date...")
            # Chuyá»ƒn text vá» Ä‘Ãºng chá»—
            df.loc[mask_swapped, 'description'] = df.loc[mask_swapped, 'published_date']
            # DÃ¹ng scraped_date trÃ¡m vÃ o chá»— trá»‘ng
            df.loc[mask_swapped, 'published_date'] = df.loc[mask_swapped, 'scraped_date']

    if 'description' in df.columns:
        df['description'] = df['description'].apply(clean_description)

    # --- 3. Xá»¬ LÃ NGÃ€Y THÃNG (LOGIC NGHIá»†P Vá»¤) ---
    today_str = datetime.now().strftime("%d/%m/%Y")
    
    # Äáº£m báº£o scraped_date khÃ´ng bá»‹ rá»—ng
    if 'scraped_date' not in df.columns:
        df['scraped_date'] = today_str
    else:
        df['scraped_date'] = df['scraped_date'].fillna(today_str)

    # Äáº£m báº£o published_date khÃ´ng bá»‹ rá»—ng (TrÃ¡m báº±ng scraped_date)
    if 'published_date' not in df.columns:
        df['published_date'] = df['scraped_date']
    else:
        df['published_date'] = df['published_date'].fillna(df['scraped_date'])

    # Kiá»ƒm tra tÃ­nh logic: published_date khÃ´ng Ä‘Æ°á»£c Lá»šN HÆ N scraped_date
    df['pub_dt'] = pd.to_datetime(df['published_date'], format='%d/%m/%Y', errors='coerce')
    df['scrape_dt'] = pd.to_datetime(df['scraped_date'], format='%d/%m/%Y', errors='coerce')
    
    mask_future_date = df['pub_dt'] > df['scrape_dt']
    anomalies = mask_future_date.sum()
    if anomalies > 0:
        print(f"âš ï¸ PhÃ¡t hiá»‡n {anomalies} ngÃ y Ä‘Äƒng vÃ´ lÃ½. Äang Ä‘á»“ng bá»™ hÃ³a...")
        df.loc[mask_future_date, 'published_date'] = df.loc[mask_future_date, 'scraped_date']
    
    df = df.drop(columns=['pub_dt', 'scrape_dt'])

    # --- 4. Lá»ŒC Bá» Dá»® LIá»†U THIáº¾U & TRÃ™NG Láº¶P ---
    required_features = ['title', 'price_billion', 'area', 'location']
    df_clean = df.dropna(subset=required_features, how='any')
    
    df_clean = df_clean.drop_duplicates(
        subset=['title', 'price_billion', 'area', 'published_date'], 
        keep='last'
    )
    print(f"âœ… Káº¿t quáº£: Giá»¯ láº¡i {len(df_clean)}/{len(df)} tin há»£p lá»‡.")

    # --- 5. LÆ¯U Káº¾T QUáº¢ ---
    conn = sqlite3.connect(config.DB_PATH)
    df_clean.to_sql('listings', conn, if_exists='replace', index=False)
    conn.close()
    
    df_clean.to_csv(config.CLEANED_DATA_PATH, index=False, encoding='utf-8-sig')
    print("ğŸ’¾ ÄÃ£ cáº­p nháº­t Database vÃ  CSV thÃ nh cÃ´ng!")

if __name__ == "__main__":
    process_and_save()