# src/data_loader/spider.py
import time
import random
import pandas as pd
from selenium.webdriver.common.by import By
import sys
import os

# Import modules t·ª´ project
# Th√™m ƒë∆∞·ªùng d·∫´n project v√†o sys.path ƒë·ªÉ import ƒë∆∞·ª£c src
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from src import config
from src.data_loader.browser import init_driver

def extract_card_data(card_element):
    """H√†m helper ƒë·ªÉ b√≥c t√°ch th√¥ng tin t·ª´ 1 th·∫ª HTML"""
    data = {}
    try:
        data['title'] = card_element.find_element(By.CSS_SELECTOR, '.js__card-title').text
    except: data['title'] = ""
    
    try: data['price'] = card_element.find_element(By.CSS_SELECTOR, '.re__card-config-price').text
    except: data['price'] = ""
    
    try: data['area'] = card_element.find_element(By.CSS_SELECTOR, '.re__card-config-area').text
    except: data['area'] = ""
    
    try: data['location'] = card_element.find_element(By.CSS_SELECTOR, '.re__card-location').text
    except: data['location'] = ""
    
    try: data['published_date'] = card_element.find_element(By.CSS_SELECTOR, '.re__card-published-info-published-at').get_attribute('aria-label')
    except: data['published_date'] = ""
    
    # L·∫§Y M√î T·∫¢ (DESCRIPTION)
    try: data['description'] = card_element.find_element(By.CSS_SELECTOR, '.re__card-description').text
    except: data['description'] = ""
    
    try: data['scraped_date'] = time.strftime("%d/%m/%Y")
    except: data['scraped_date'] = ""

    return data

def run_crawler(pages=2):
    driver = init_driver()
    results = []
    
    try:
        for p in range(1, pages + 1):
            url = config.BASE_URL if p == 1 else f"{config.BASE_URL}/p{p}"
            print(f"[Spider] ƒêang c√†o trang {p}/{pages}: {url}")
            
            try:
                driver.get(url)
                time.sleep(random.uniform(5, 8))

                if p == 1:
                    screenshot_path = os.path.join(config.DATA_DIR, 'debug_github_actions.png')
                    driver.save_screenshot(screenshot_path)
                    print(f"üì∏ ƒê√£ ch·ª•p ·∫£nh m√†n h√¨nh t·∫°i: {screenshot_path}")
                
                cards = driver.find_elements(By.CSS_SELECTOR, '.js__card')
                for card in cards:
                    # Logic b√≥c t√°ch c≈© c·ªßa b·∫°n
                    try:
                        data = extract_card_data(card)
                        print(f"[Spider] B√≥c t√°ch: {data['title']} | {data['price']} | {data['area']} | {data['location']} | {data['published_date']}")
                        if data['title']:  # Ch·ªâ l·∫•y tin c√≥ ti√™u ƒë·ªÅ
                            results.append(data)
                    except: continue
                    
            except Exception as e:
                print(f"[Spider] L·ªói t·∫°i trang {p}: {e}")
                
    finally:
        try: driver.quit()
        except: pass
        
    return results

def save_data(data):
    if not data:
        print("[Spider] Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi.")
        return

    header = not os.path.exists(config.RAW_CSV_PATH)
    df = pd.DataFrame(data)
    df.to_csv(config.RAW_CSV_PATH, mode='a', index=False, header=header, encoding='utf-8-sig')
    print(f"[Spider] ƒê√£ l∆∞u {len(df)} d√≤ng v√†o: {config.RAW_CSV_PATH}")

if __name__ == "__main__":
    # ƒêi·ªÉm ch·∫°y test
    data = run_crawler(pages=100)
    save_data(data)